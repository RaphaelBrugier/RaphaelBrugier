<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Raphael Brugier</title>
    <link>https://www.raphael-brugier.com/categories/programming/index.xml</link>
    <description>Recent content on Raphael Brugier</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://www.raphael-brugier.com/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>5 laws every developer should know</title>
      <link>https://www.raphael-brugier.com/blog/5-laws-every-developer-should-know/</link>
      <pubDate>Thu, 17 Aug 2017 17:18:44 -0400</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/5-laws-every-developer-should-know/</guid>
      <description>&lt;p&gt;Laws - or principles - can give us guidance and teach us lessons from our peers’ mistakes. In this article, I will introduce you to five laws I always have in the back of my mind when designing or implementing a software. Some of them relate to pure development, some are related to system organizations. All of them should be useful for your growth as a software engineer.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;murphy-s-law&#34;&gt;Murphy’s Law&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;If anything can go wrong, it will.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Murphy%27s_law&#34;&gt;This law&lt;/a&gt; was coined by Edward Murphy - an aerospace engineer - in response to a failed rocket test in the early 50’s.
The idea captured in this law is to always create a defensive design for the critical parts of your system… because something will eventually go wrong at some point!&lt;/p&gt;

&lt;p&gt;This law is easily translated to the software engineering field.
When you expose the software to the end-users, they will find creative ways to input something you had not planned and break the system. So you need to make your software is robust enough to detect and alert for unexpected behavior.&lt;/p&gt;

&lt;p&gt;When you run the software on a machine, anything can break - from the disks supporting the OS to the data center’s electrical supply. So you need to make sure you have designed for failures at all levels of your architecture.&lt;/p&gt;

&lt;p&gt;I have had the chance to meet Murphy’s law several times already.
For example, I did not think using the default value “null” to represent null Strings in the batch framework I was using was harmful until someone actually named “Null” passed a trade order and broke our report chain for several hours…
Or, on another project, everything seemed ready to deploy the production environment until Azure had an infrastructure incident which took down the server we used to run the automation scripts.
These real-world lessons reminded me the hard way that if anything can go wrong, it will.&lt;/p&gt;

&lt;p&gt;So, always keep Murphy in mind and design robust software.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://www.raphael-brugier.com/img/2017/08/Murphys-law.jpg&#34;&gt;
&lt;/p&gt;

&lt;h1 id=&#34;knuth-s-law&#34;&gt;Knuth’s Law&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;“Premature optimization is the root of all evil (or at least most of it) in programming.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikiquote.org/wiki/Donald_Knuth#Quotes&#34;&gt;This law&lt;/a&gt; - or I should say one of the most famous quotes from Donald Knuth - reminds us that you should never try to optimize the code of an application too early, or until it is actually necessary.&lt;/p&gt;

&lt;p&gt;Indeed, a source code produced with simplicity and readability in mind will suffice for 99% of the performance needs and will greatly improve the maintainability of an application. Starting with a simpler solution will also make easier to iterate and improve when a performance problem arises.&lt;/p&gt;

&lt;p&gt;Strings concatenation is often an example of a premature optimization for garbage collected languages. In Java or C#, Strings are immutable and we are taught to use other structures to build Strings dynamically, like a &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/lang/StringBuilder.html&#34;&gt;StringBuilder&lt;/a&gt;. But in reality, until you have profiled the application, you don’t really know how many times a String is going to be created and what is the performance impact. So it often makes more sense to write it first with the clearest code possible and later optimize if necessary.&lt;/p&gt;

&lt;p&gt;However, this rule should not prevent from learning the performance trade offs of your language and when to use the correct data structures.
And, like with every other performance problem, you should always &lt;em&gt;measure first&lt;/em&gt; before starting to optimize anything.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://www.raphael-brugier.com/img/2017/08/performance-timer.jpg&#34;&gt;
&lt;/p&gt;

&lt;h1 id=&#34;north-s-law&#34;&gt;North’s Law&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;“Every decision is a trade off.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ok, I admit this quote from one of Dan North’s talks - &lt;a href=&#34;https://www.youtube.com/watch?v=EauykEv_2iA&#34;&gt;Decisions, Decisions&lt;/a&gt; - is not (yet!) recognized as a law.
But this quote has had such an impact on the way I approach all my decisions I thought I should include it here.&lt;/p&gt;

&lt;p&gt;In the day to day life of a developer, we make a ton of decisions - whether big or small - every day. From naming a variable to defining the architecture of a platform, through automating (or not) tasks.&lt;/p&gt;

&lt;p&gt;This quote emphasizes that whatever choice you are making, you are always giving up on something, one or more options.
But that’s not the most important.
The most important is to consciously make a decision and being aware of the other options and why you did not choose them. You should always thrive to make a decision by weighing the pros and cons based on what you know at that moment.
But it should also be fine to discover later that a decision you took was wrong if you new information comes to you after. The critical thing is to remember why you took the decision, reevaluate the new options and make a new conscious choice.&lt;/p&gt;

&lt;p&gt;Again.
&amp;gt; &amp;ldquo;Every decision is a trade off.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;So make choices and raise your awareness of your options.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://www.raphael-brugier.com/img/2017/08/balance.png&#34;&gt;
&lt;/p&gt;

&lt;h1 id=&#34;conway-s-law&#34;&gt;Conway’s Law&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;“Organizations which design systems &amp;hellip; are constrained to produce designs which are copies of the communication structures of these organizations.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the 60s, an engineer named Melvin Conway noticed that how organizations are structured influences the design of the systems they produce.
He described this idea in a paper and the name stuck as “&lt;a href=&#34;https://en.wikipedia.org/wiki/Conway%27s_law&#34;&gt;Conway’s law&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;This law translates well into the software development world and is even reflected at the code level. The way teams are organized to deliver software components will directly influence the design of each component.
For example, a collocated team of developers will produce a monolithic application with coupled components. On the other hand, multiple distributed teams will produce multiple separated (micro) services with a clearer separation of concern for each.
Neither design is good or bad, but they have both been influenced by the way the team(s) communicate.
Open source projects, with multiple individuals around the globe, are often great examples of modularity and reusable libraries.&lt;/p&gt;

&lt;p&gt;Nowadays, the current trend is to break monolithic applications into micro-services. This is awesome and it will enable more velocity to deliver projects faster into production. But you should always keep in mind the Conway’s law and work as much on the organization of your company as on the technology choices.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;https://www.raphael-brugier.com/img/2017/08/PreferFunctionalStaffOrganization.png&#34;&gt;
&lt;/p&gt;

&lt;h1 id=&#34;law-of-triviality-parkinson-s-law-of-triviality&#34;&gt;Law of triviality (Parkinson&amp;rsquo;s law of triviality)&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;“Members of an organization give disproportionate weight to trivial issues.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The argument of &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_triviality&#34;&gt;this law&lt;/a&gt; is that the time spent on any item of a meeting agenda is in inverse proportion to the sum of money involved.
Indeed, people have a tendency to give more attention to a subject they fully understand and have an opinion about than a complex problem.&lt;/p&gt;

&lt;p&gt;Parkinson gives the example of a meeting during which a committee is reviewing two decisions: building a nuclear reactor for the company and building a bikeshed for the employees. Constructing a reactor is a vast and complicated task and people cannot grasp it entirely. Instead, they fully rely on their processes and system experts and quickly accept the project.
On the other hand, building a bikeshed is something that an average person can do and everyone can have an opinion on the color. In fact, every committee member will make sure to voice his opinion and the bikeshed decision will proportionally take way more time than the reactor’s.&lt;/p&gt;

&lt;p&gt;This law has been popularized in the software world - and named after this story as the &lt;a href=&#34;https://en.wiktionary.org/wiki/bikeshedding&#34;&gt;bike-shed effect&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Developers, for example, can spend more time discussing the correct indentation or naming of a function than actually discussing the responsibility of a class or the architecture of an application. That’s because, again, everyone can picture the effect of a few characters changes but it takes a bigger cognitive load to project an architecture change.&lt;/p&gt;

&lt;p&gt;Another place where you will notice a lot of bikeshed effects are Scrum demos.
Don’t get me wrong, I love demos and I think that’s a great opportunity to face the users and get feedback on an application.
But often, the discussion during a Scrum demo will slip to cosmetic questions and specific behaviors instead of looking at the bigger picture. These discussions are also important but you have to be careful to balance these with the most important - and complicated - problems.&lt;/p&gt;

&lt;p&gt;Once you know the pattern, you will start noticing this behavior in a lot of meetings and people interactions.
I am not telling you to cut every discussion about “small” problems, but raising your awareness will help you focus on the real problems and be better prepared for these meetings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/2017/08/pink-bikeshed.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;These 5 laws are only a few examples of old lessons learned in our industry. There are many more to learn and discover when gaining more experience in the software development trenches.&lt;/p&gt;

&lt;p&gt;Even if some of them may be seen as common-sense now, I strongly believe that knowing these principles will help you recognize patterns and react to them.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fix the Git completion with Oh-my-Zsh on Mac</title>
      <link>https://www.raphael-brugier.com/blog/fix-git-completion-zsh-mac-homebrew/</link>
      <pubDate>Sun, 16 Jul 2017 22:34:32 -0400</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/fix-git-completion-zsh-mac-homebrew/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://www.raphael-brugier.com/blog/my-development-environment/&#34;&gt;previous post&lt;/a&gt;, I have explained how I have setup oh-my-zsh with the git plugin.
I am also using homebrew to manage the packages installed on my Mac.
After upgrading Git recently, I have noticed the Git completion was not as powerful anymore.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;To explain how the Zsh completion is cool and supposed to be, here is an example of a correct completion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git commit --&amp;lt;tab&amp;gt;
--all                           -- stage all modified and deleted paths
--allow-empty                   -- allow recording an empty commit
--allow-empty-message           -- allow recording a commit with an empty message
--amend                         -- amend the tip of the current branch
--author                        -- override the author name used in the commit
--cleanup                       -- specify how the commit message should be cleaned up
--date                          -- override the author date used in the commit
--dry-run                       -- only show list of paths that are to be commited or not, and any untracked
--edit                          -- edit the commit message before committing
--file                          -- read commit message from given file
--gpg-sign                      -- GPG-sign the commit
--include                       -- update the given files and commit the whole index
--interactive                   -- interactively update paths in the index file
--message                       -- use the given message as the commit message
--no-edit                       -- do not edit the commit message before committing
--no-gpg-sign                   -- do not GPG-sign the commit
--no-post-rewrite               -- bypass the post-rewrite hook
--no-status                     -- do not include the output of git status in the commit message template
--no-verify                     -- do not look for suspicious lines the commit introduces
--null                          -- separate dry run entry output with NUL
--only                          -- commit only the given files
--patch                         -- use the interactive patch selection interface to chose which changes to commit
--porcelain                     -- output dry run in porcelain-ready format
--quiet                         -- suppress commit summary message
--reedit-message                -- use existing commit object and edit log message
--reuse-message                 -- use existing commit object with same log message
--short                         -- output dry run in short format
--signoff                       -- add Signed-off-by line at the end of the commit message
--squash               --fixup  -- construct a commit message for use with rebase --autosquash
--status                        -- include the output of git status in the commit message template
--template                      -- use file as a template commit message
--untracked-files               -- show files in untracked directories
--verbose                       -- show unified diff of all file changes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The completion displays all the parameters for a Git command with a short documentation.&lt;/p&gt;

&lt;p&gt;After updating Git with the &lt;code&gt;brew update&lt;/code&gt; command here is what it looked like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$git commit --&amp;lt;tab&amp;gt;
--all               --author=           --dry-run           --fixup=            --message=
--only              --reedit-message=   --short             --template=         --verbose
--allow-empty       --cleanup=          --edit              --include           --no-edit
--patch             --reset-author      --signoff           --untracked-files   --verify
--amend             --date              --file=             --interactive       --no-verify
--quiet             --reuse-message=    --squash=           --untracked-files=
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yep, all the integrated documentation was gone :(
I was preparing for a Git training at Ippon and I was disappointed not to be able to show this very useful feature to my colleagues new to Git.&lt;/p&gt;

&lt;p&gt;It turned that the culprit is homebrew installing a completion for Git which overrides the Zsh completion.
You can verify this with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ls -l /usr/local/etc/bash_completion.d
total 64
lrwxr-xr-x  1 raphael  admin  64 Oct 21  2016 tig-completion.bash -&amp;gt; ../../Cellar/tig/2.2_1/etc/bash_completion.d/tig-completion.bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The solution is to reinstall git with homebrew and specify not to install the bash completion.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ brew uninstall git
$ brew install git --without-completions
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And voilà!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>remove known_hosts entry with ssh keygen</title>
      <link>https://www.raphael-brugier.com/til/remove-known_hosts-entry-with-ssh-keygen/</link>
      <pubDate>Mon, 03 Jul 2017 09:26:07 -0400</pubDate>
      
      <guid>https://www.raphael-brugier.com/til/remove-known_hosts-entry-with-ssh-keygen/</guid>
      <description>&lt;p&gt;The easiest way to remove an entry from the &lt;code&gt;.ssh/known_hosts&lt;/code&gt; file is to use &lt;code&gt;ssh-keygen&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-keygen -R hostname
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>MongoDB and Apache Spark - Getting started tutorial</title>
      <link>https://www.raphael-brugier.com/blog/mongodb-apache-spark-getting-started-tutorial/</link>
      <pubDate>Wed, 03 May 2017 23:01:48 -0400</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/mongodb-apache-spark-getting-started-tutorial/</guid>
      <description>&lt;p&gt;MongoDB and Apache Spark are two popular Big Data technologies.&lt;/p&gt;

&lt;p&gt;In my &lt;a href=&#34;https://www.raphael-brugier.com/blog/introduction-mongodb-spark-connector/&#34;&gt;previous post&lt;/a&gt;, I listed the capabilities of the &lt;a href=&#34;https://docs.mongodb.com/spark-connector/v2.0/&#34;&gt;MongoDB connector for Spark&lt;/a&gt;. In this tutorial, I will show you how to configure Spark to connect to MongoDB, load data, and write queries.&lt;/p&gt;

&lt;p&gt;To demonstrate how to use Spark with MongoDB, I will use the zip codes from MongoDB tutorial on &lt;a href=&#34;https://docs.mongodb.com/v3.2/tutorial/aggregation-zip-code-data-set/&#34;&gt;the aggregation pipeline documentation using a zip code data set&lt;/a&gt;. I have prepared a Maven project and a Docker Compose file to get you started quickly.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Install &lt;a href=&#34;https://docs.docker.com/engine/installation/&#34;&gt;Docker and Docker Compose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install Maven&lt;/li&gt;
&lt;li&gt;Download the project &lt;a href=&#34;https://github.com/raphaelbrugier/spark-mongo-example&#34;&gt;from Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the project root, launch the MongoDB server with docker-compose:
&lt;code&gt;docker-compose -f docker/docker-compose.yml up -d&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Import the data in the MongoDB database running in the container:
&lt;code&gt;docker exec -it mongo_container sh /scripts/import-data.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Check that the data has been loaded in MongoDB by connecting to the container and running a count:
&lt;code&gt;docker exec mongo_container mongo --eval &amp;quot;db.zips.count()&amp;quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This should return:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;MongoDB shell version: 3.2.11
connecting to: test
29353
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The zips collection is a collection of &lt;em&gt;Document&lt;/em&gt; with the following model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;_id&amp;quot;: &amp;quot;10280&amp;quot;,
  &amp;quot;city&amp;quot;: &amp;quot;NEW YORK&amp;quot;,
  &amp;quot;state&amp;quot;: &amp;quot;NY&amp;quot;,
  &amp;quot;pop&amp;quot;: 5574,
  &amp;quot;loc&amp;quot;: [
    -74.016323,
    40.710537
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Import the Maven project in your favorite IDE. Create a new file &lt;em&gt;Main.scala&lt;/em&gt; to copy the examples or run the &lt;code&gt;MongoSparkMain&lt;/code&gt; for the solution.&lt;/p&gt;

&lt;h2 id=&#34;read-data-from-mongodb-to-spark&#34;&gt;Read data from MongoDB to Spark&lt;/h2&gt;

&lt;p&gt;In this example, we will see how to configure the connector and read from a MongoDB collection to a DataFrame.&lt;/p&gt;

&lt;p&gt;First, you need to create a minimal SparkContext, and then to configure the &lt;code&gt;ReadConfig&lt;/code&gt; instance used by the connector with the MongoDB URL, the name of the database and the collection to load:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import com.mongodb.spark._
import com.mongodb.spark.config.ReadConfig
import com.mongodb.spark.sql._
import com.typesafe.scalalogging.slf4j.LazyLogging
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{max, min}
import org.bson.Document

object Main extends App with LazyLogging {
    val spark = SparkSession.builder()
    .appName(&amp;quot;mongozips&amp;quot;)
    .master(&amp;quot;local[*]&amp;quot;)
    .getOrCreate()

  // Read the data from MongoDB to a DataFrame
  val readConfig = ReadConfig(Map(&amp;quot;uri&amp;quot; -&amp;gt; &amp;quot;mongodb://127.0.0.1/&amp;quot;, &amp;quot;database&amp;quot; -&amp;gt; &amp;quot;test&amp;quot;, &amp;quot;collection&amp;quot; -&amp;gt; &amp;quot;zips&amp;quot;)) // 1)
  val zipDf = spark.read.mongo(readConfig) // 2)
  zipDf.printSchema() // 3)
  zipDf.show()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Set the MongoDB URL, database, and collection to read.&lt;/li&gt;
&lt;li&gt;The connector provides a method to convert a MongoRDD to a DataFrame. The DataFrame’s schema is automatically inferred by the connector by &lt;a href=&#34;https://docs.mongodb.com/v3.2/reference/operator/aggregation/sample/&#34;&gt;sampling&lt;/a&gt; the collection. Alternatively, you can explicitly pass a schema definition.&lt;/li&gt;
&lt;li&gt;Print the schema inferred by the connector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;root
 |-- _id: string (nullable = true)
 |-- city: string (nullable = true)
 |-- loc: array (nullable = true)
 |    |-- element: double (containsNull = true)
 |-- pop: integer (nullable = true)
 |-- state: string (nullable = true)

+-----+-----------+--------------------+-----+-----+
|  _id|       city|                 loc|  pop|state|
+-----+-----------+--------------------+-----+-----+
|01001|     AGAWAM|[-72.622739, 42.0...|15338|   MA|
|01002|    CUSHMAN|[-72.51565, 42.37...|36963|   MA|
|01005|      BARRE|[-72.108354, 42.4...| 4546|   MA|
|01007|BELCHERTOWN|[-72.410953, 42.2...|10579|   MA|
|01008|  BLANDFORD|[-72.936114, 42.1...| 1240|   MA|
+-----+-----------+--------------------+-----+-----+
only showing top 5 rows
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The connector has correctly inferred the schema based on the documents sampling. Both the column names and types have been identified accurately.&lt;/p&gt;

&lt;h3 id=&#34;inferring-inner-documents&#34;&gt;Inferring inner-documents&lt;/h3&gt;

&lt;p&gt;Interestingly, the &lt;code&gt;loc&lt;/code&gt; array from the MongoDB document has been translated to a Spark’s Array type.&lt;/p&gt;

&lt;p&gt;But what if the document contains inner documents? The connector does not flatten the inner document but translates them as a Spark’s StructType, a key-value type.&lt;/p&gt;

&lt;p&gt;Take for example this MongoDB document:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  name: &amp;quot;Joe Bookreader&amp;quot;,
  country: {
    isocode: &amp;quot;USA&amp;quot;,
    name: &amp;quot;United States&amp;quot;
  },
  addresses: [
    {
      street: &amp;quot;123 Fake Street&amp;quot;,
      city: &amp;quot;Faketon&amp;quot;,
      state: &amp;quot;MA&amp;quot;,
      zip: &amp;quot;12345&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The document has two inner documents. The first one is the country and the second one is an address contained in a list.&lt;/p&gt;

&lt;p&gt;After loading the collections, the schema inferred by the connector shows a StructType for both the &lt;code&gt;country&lt;/code&gt; and the &lt;code&gt;address&lt;/code&gt; in the array of addresses.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val personDf = spark.read.mongo(ReadConfig(Map(&amp;quot;uri&amp;quot; -&amp;gt; &amp;quot;mongodb://127.0.0.1/&amp;quot;, &amp;quot;database&amp;quot; -&amp;gt; &amp;quot;test&amp;quot;, &amp;quot;collection&amp;quot; -&amp;gt; &amp;quot;person&amp;quot;)))
personDf.printSchema()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;root
 |-- _id: struct (nullable = true)
 |    |-- oid: string (nullable = true)
 |-- addresses: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- street: string (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- zip: string (nullable = true)
 |-- country: struct (nullable = true)
 |    |-- isocode: string (nullable = true)
 |    |-- name: string (nullable = true)
 |-- name: string (nullable = true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The values in the StructType types can be accessed by their column names:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;personDf.select($&amp;quot;_id&amp;quot;, $&amp;quot;addresses&amp;quot;(0)(&amp;quot;street&amp;quot;), $&amp;quot;country&amp;quot;(&amp;quot;name&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find the list of the mappings between the MongoDB types and the DataFrame’s types in the &lt;a href=&#34;https://docs.mongodb.com/spark-connector/v2.0/scala/datasets-and-sql/#datatypes&#34;&gt;connector&amp;rsquo;s documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;use-the-spark-api-to-query-the-data&#34;&gt;Use the Spark API to query the data&lt;/h2&gt;

&lt;p&gt;After loading the collection in a DataFrame, we can now use the Spark API to query and transform the data.&lt;/p&gt;

&lt;p&gt;As an example, we write a query to find the states with a population greater or equal to 10 million. The example also shows how the Spark API can easily map to the original MongoDB query.&lt;/p&gt;

&lt;p&gt;The MongoDB query is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;db.zipcodes.aggregate( [
   { $group: { _id: &amp;quot;$state&amp;quot;, totalPop: { $sum: &amp;quot;$pop&amp;quot; } } },
   { $match: { totalPop: { $gte: 10*1000*1000 } } }
] )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now the Spark query:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;println( &amp;quot;States with Populations above 10 Million&amp;quot; )
import zipDf.sqlContext.implicits._ // 1)
zipDf.groupBy(&amp;quot;state&amp;quot;)
    .sum(&amp;quot;pop&amp;quot;)
    .withColumnRenamed(&amp;quot;sum(pop)&amp;quot;, &amp;quot;count&amp;quot;) // 2)
    .filter($&amp;quot;count&amp;quot; &amp;gt; 10000000)
    .show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Result:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;States with Populations above 10 Millions:
+-----+--------+
|state|   count|
+-----+--------+
|   TX|16984601|
|   NY|17990402|
|   OH|10846517|
|   IL|11427576|
|   CA|29754890|
|   PA|11881643|
|   FL|12686644|
+-----+--------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The DataFrame API is pretty straight forward for this simple query.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use the import to have implicit conversions from &lt;code&gt;String&lt;/code&gt; to &lt;code&gt;Column&lt;/code&gt; with the &lt;code&gt;$&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Rename the result of the sum column for readability.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;spark-sql&#34;&gt;Spark SQL&lt;/h2&gt;

&lt;p&gt;Spark and the DataFrame abstraction also enables to write plain Spark SQL queries with a familiar SQL syntax.&lt;/p&gt;

&lt;p&gt;For example, let’s rewrite the previous query with SQL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// SparkSQL:
  zipDf.createOrReplaceTempView(&amp;quot;zips&amp;quot;) // 1)
  zipDf.sqlContext.sql( // 2)
    &amp;quot;&amp;quot;&amp;quot;SELECT state, sum(pop) AS count
      FROM zips
      GROUP BY state
      HAVING sum(pop) &amp;gt; 10000000&amp;quot;&amp;quot;&amp;quot;
  )
  .show()
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Register the DataFrame as a Spark SQL table.&lt;/li&gt;
&lt;li&gt;Execute the Spark SQL query.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;predicates-pushdown&#34;&gt;Predicates pushdown&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;“Predicates pushdown”&lt;/em&gt; is an optimization from the connector and the Catalyst optimizer to automatically “push down” predicates to the data nodes. The goal is to maximize the amount of data filtered out on the data storage side before loading it into Spark’s node memory.&lt;/p&gt;

&lt;p&gt;There are two kinds of predicates automatically pushed down by the connector to MongoDB:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;select&lt;/code&gt; clause (projections) as a &lt;a href=&#34;https://docs.mongodb.com/manual/reference/operator/aggregation/project/&#34;&gt;&lt;code&gt;$project&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the &lt;code&gt;filter&lt;/code&gt; clause content (&lt;code&gt;where&lt;/code&gt;) as one or more &lt;a href=&#34;https://docs.mongodb.com/manual/reference/operator/aggregation/match/&#34;&gt;&lt;code&gt;$match&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both are sent by the connector as an &lt;a href=&#34;https://github.com/mongodb/mongo-spark/blob/master/src/main/scala/com/mongodb/spark/sql/MongoRelationHelper.scala#L30&#34;&gt;aggregation pipeline&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To verify if the predicates are sent, we use the Spark’s &lt;em&gt;explain&lt;/em&gt; method to examine the query plan produced by Spark for a simple query with a filter:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;zipDf
    .filter($&amp;quot;pop&amp;quot; &amp;gt; 0)
    .select(&amp;quot;state&amp;quot;)
    .explain(true)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;== Parsed Logical Plan ==
[...]

== Analyzed Logical Plan ==
[...]

== Optimized Logical Plan ==
[...]

== Physical Plan ==
*Project [state#4]
+- *Filter (isnotnull(pop#3) &amp;amp;&amp;amp; (pop#3 &amp;gt; 0))
   +- *Scan MongoRelation(MongoRDD[0] at RDD at MongoRDD.scala:52,Some(StructType(StructField(_id,StringType,true), StructField(city,StringType,true), StructField(loc,ArrayType(DoubleType,true),true), StructField(pop,IntegerType,true), StructField(state,StringType,true)))) [state#4,pop#3] PushedFilters: [IsNotNull(pop), GreaterThan(pop,0)], ReadSchema: struct&amp;lt;state:string&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see in the physical plan generated by the Catalyst optimizer, the name of the fields to project (&lt;code&gt;state&lt;/code&gt; and &lt;code&gt;pop&lt;/code&gt;), and the filters to push (pop not null and pop greater than 0).&lt;/p&gt;

&lt;p&gt;To confirm what is actually executed on the MongoDB nodes, we need to increase MongoDB’s log level and examine the &lt;code&gt;system.profile&lt;/code&gt; collection.&lt;/p&gt;

&lt;p&gt;Enable the logging on MongoDB, run the Spark query again, and find the trace of the query in the &lt;code&gt;system.profile&lt;/code&gt; collection:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$mongo
MongoDB shell version: 3.2.11
connecting to: test

&amp;gt; db.setProfilingLevel(2)
&amp;gt; db.system.profile.find().pretty()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
	&amp;quot;op&amp;quot; : &amp;quot;command&amp;quot;,
	&amp;quot;ns&amp;quot; : &amp;quot;test.zips&amp;quot;,
	&amp;quot;command&amp;quot; : {
		&amp;quot;aggregate&amp;quot; : &amp;quot;zips&amp;quot;,
		&amp;quot;pipeline&amp;quot; : [
			{
				&amp;quot;$match&amp;quot; : {
					&amp;quot;_id&amp;quot; : {
						&amp;quot;$gte&amp;quot; : { &amp;quot;$minKey&amp;quot; : 1 },
						&amp;quot;$lt&amp;quot; : { &amp;quot;$maxKey&amp;quot; : 1 }
					}
				}
			},
			{
				&amp;quot;$match&amp;quot; : {
					&amp;quot;pop&amp;quot; : {
						&amp;quot;$exists&amp;quot; : true,
						&amp;quot;$ne&amp;quot; : null,
						&amp;quot;$gt&amp;quot; : 0
					}
				}
			},
			{
				&amp;quot;$project&amp;quot; : {
					&amp;quot;state&amp;quot; : 1,
					&amp;quot;pop&amp;quot; : 1,
					&amp;quot;_id&amp;quot; : 0
				}
			}
		],
		&amp;quot;cursor&amp;quot; : {

		},
		&amp;quot;allowDiskUse&amp;quot; : true
	},
	[...]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result shows the &lt;code&gt;$project&lt;/code&gt; and &lt;code&gt;$match&lt;/code&gt; clauses executed by MongoDB and, as expected, they match the Spark’s physical plan.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, I have shown how to connect to a MongoDB database with Apache Spark to load and query the data. The connector provides a set of utility methods to easily load data from MongoDB to a DataFrame.&lt;/p&gt;

&lt;p&gt;I have also presented how a MongoDB &lt;em&gt;Document&lt;/em&gt; is mapped to a Spark’s DataFrame. Because of the hierarchical nature of a &lt;em&gt;Document&lt;/em&gt;, only the first level of attributes is mapped to columns. Inner documents become nested columns.&lt;/p&gt;

&lt;p&gt;Finally, I have described how the connector minimizes the data loaded in Spark by taking advantage of the predicates pushdown optimization, an essential feature of every connector. The connector does a good job of sending the predicates automatically, but it is helpful to know how to confirm if and how the predicates are applied on the MongoDB side.&lt;/p&gt;

&lt;p&gt;In conclusion, the connector is fully functional to take benefit from using Spark on top of MongoDB.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduction to the MongoDB connector for Apache Spark</title>
      <link>https://www.raphael-brugier.com/blog/introduction-mongodb-spark-connector/</link>
      <pubDate>Fri, 31 Mar 2017 23:08:27 -0400</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/introduction-mongodb-spark-connector/</guid>
      <description>&lt;p&gt;MongoDB is one of the most popular NoSQL databases. Its unique capabilities to store document-oriented data using the built-in sharding and replication features provide horizontal scalability as well as high availability.&lt;/p&gt;

&lt;p&gt;Apache Spark is another popular “Big Data” technology. Spark provides a lower entry level to the world of distributed computing by offering an easier to use, faster, and in-memory framework than the MapReduce framework. Apache Spark is intended to be used with any distributed storage, e.g. HDFS, Apache Cassandra with the &lt;a href=&#34;https://github.com/datastax/spark-cassandra-connector&#34;&gt;Datastax’s spark-cassandra-connector&lt;/a&gt; and now the &lt;a href=&#34;https://docs.mongodb.com/spark-connector/v2.0/&#34;&gt;MongoDB&amp;rsquo;s connector&lt;/a&gt; presented in this article.&lt;/p&gt;

&lt;p&gt;By using Apache Spark as a data processing platform on top of a MongoDB database, you can benefit from all of the major Spark API features: the RDD model, the SQL (HiveQL) abstraction and the Machine Learning libraries.&lt;/p&gt;

&lt;p&gt;In this article, I present the features of the connector and some use cases. An upcoming article will be a tutorial to demonstrate how to load data from MongoDB and run queries with Spark.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;mongodb-connector-for-spark-features&#34;&gt;“MongoDB connector for Spark” features&lt;/h2&gt;

&lt;p&gt;The MongoDB connector for Spark is an open source project, written in Scala, to read and write data from MongoDB using Apache Spark.&lt;/p&gt;

&lt;p&gt;The latest version - 2.0 - supports MongoDB &amp;gt;=2.6 and Apache Spark &amp;gt;= 2.0. The previous version - 1.1 - supports MongoDB &amp;gt;= 2.6 and Apache Spark &amp;gt;= 1.6 this is the version used in &lt;a href=&#34;https://university.mongodb.com/courses/M233/about&#34;&gt;the MongoDB online course&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The connector offers various features, most importantly:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ability to read/write BSON documents directly from/to MongoDB.&lt;/li&gt;
&lt;li&gt;Converting a MongoDB collection into a Spark RDD.&lt;/li&gt;
&lt;li&gt;Utility methods to load collections directly into a Spark DataFrame or DataSet.&lt;/li&gt;
&lt;li&gt;Predicates pushdown:

&lt;ul&gt;
&lt;li&gt;Predicates pushdown is an optimization from the Spark SQL&amp;rsquo;s Catalyst optimizer to push the &lt;code&gt;where&lt;/code&gt; filters and the &lt;code&gt;select&lt;/code&gt; projections down to the datasource.
With MongoDB as the datasource, the connector will convert the Spark&amp;rsquo;s filters to a MongoDB aggregation pipeline &lt;code&gt;match&lt;/code&gt;.
As a result, the actual filtering and projections are done on the MongoDB node before returning the data to the Spark node.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Integration with the MongoDB aggregation pipeline:

&lt;ul&gt;
&lt;li&gt;The connector accepts MongoDB&amp;rsquo;s pipeline definitions on a MongoRDD to execute aggregations on the MongoDB nodes instead of the Spark nodes.
In reality, with most of the work to optimize the data load in the workers done automatically by the connector it should be used in rare cases.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Data locality:

&lt;ul&gt;
&lt;li&gt;If the Spark nodes are deployed on the same nodes as the MongoDB nodes, and correctly configured with a &lt;code&gt;MongoShardedPartitioner&lt;/code&gt;, then the Spark nodes will load the data according to their locality in the cluster. This will avoid costly network transfers when first loading the data in the Spark nodes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/2017/03/mongodbsparkconnector.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Spark nodes deployed next to the MongoDB nodes&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;use-cases&#34;&gt;Use cases&lt;/h2&gt;

&lt;p&gt;Different use cases can apply to run Spark on top of a MongoDB database, but they all take advantage of MongoDB’s built-in replication and sharding mechanisms to run Spark on the same large MongoDB cluster used by the business applications to store their data.&lt;/p&gt;

&lt;p&gt;Typically, applications read/write on the primary replica set while the Spark nodes read data from a secondary replica set.&lt;/p&gt;

&lt;p&gt;To provide analytics, Spark can be used to extract data from MongoDB, run complex queries and then write the data back to another MongoDB collection. This has the benefit to not introduce a new data storage while using the processing power of Spark.&lt;/p&gt;

&lt;p&gt;If there is already a centralized storage - a Data Lake, for instance, built with HDFS - Spark can extract and transform data from MongoDB before writing it to HDFS. The advantage is to use Spark as a simple and effective ETL tool to move the data from MongoDB to the data lake.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article, I have listed the MongoDB connector features and use cases.&lt;/p&gt;

&lt;p&gt;The connector is fully functional and provides a set of utility methods to simplify the interactions between Spark and MongoDB.&lt;/p&gt;

&lt;p&gt;Data locality - the ability to load the data on Spark nodes based on their MongoDB shard location - is another optimization from the MongoDB connector but requires extra configuration and is beyond the scope of this article.&lt;/p&gt;

&lt;p&gt;In the next post, I will give a practical tour with code examples on how to connect Spark to MongoDB and write queries.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Add a Git hook to automatically verify a repository&#39;s email</title>
      <link>https://www.raphael-brugier.com/blog/git-verify-email-hook/</link>
      <pubDate>Sat, 03 Dec 2016 13:40:15 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/git-verify-email-hook/</guid>
      <description>&lt;p&gt;I use Git a lot and I often have to switch between my personal repositories (ie: Github) and my professional (Ippon) repositories on the same laptop.
My default Git email is configured to my personal email and I have often forgotten to configure it to my professional email when creating/cloning a repository for my company.
Like everything in Git, this can be automated to avoid mistakes.&lt;/p&gt;

&lt;p&gt;In this post, I will show how I use a Git hook to check the email configured in any repository before every commit.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;email-configuration-per-repositories&#34;&gt;Email configuration per repositories:&lt;/h1&gt;

&lt;p&gt;When you first installed Git you probably have set your username and email globally with these commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git config --global user.name &amp;quot;Raphael Brugier&amp;quot;
$ git config --global user.email &amp;quot;raphael.b_____r@gmail.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can override this values per repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd some-ippon-project
$ git config user.email &amp;quot;rbrugier@ipponusa.com&amp;quot;
## print the configuration
$ git config user.email
&amp;gt; rbrugier@ipponusa.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;git-hooks&#34;&gt;Git hooks&lt;/h1&gt;

&lt;p&gt;Git hooks are scripts that Git executes before or after Git commands.
For example: commit, push, rebase, &amp;hellip;&lt;/p&gt;

&lt;p&gt;They are located in every Git repository in the .git/hooks/ folder.
By default, Git creates hooks suffixed with &amp;ldquo;.sample&amp;rdquo;
To create a custom hook, create a new executable script without the suffix for the hook you want to create and add the Bash code for your needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim .git/hooks/pre-commit
## add the hook code as any bash script
$ chmod +x .git/hooks/pre-commit
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;global-git-hooks&#34;&gt;Global Git hooks&lt;/h1&gt;

&lt;p&gt;Since &lt;a href=&#34;https://github.com/blog/2188-git-2-9-has-been-released&#34;&gt;its version 2.9&lt;/a&gt;, Git offers an option to share the hooks globally between the repositories. I prefer this option to duplicating hooks in every repository, even if you could use templates to automatically copy them in every new repository.&lt;/p&gt;

&lt;p&gt;Create a hooks folder, for example in your &lt;a href=&#34;https://www.raphael-brugier.com/blog/my-development-environment/&#34;&gt;dotfiles&lt;/a&gt;, and configure Git globally to use this folder as the hooks folder for every repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir -p ~/dotfiles/_git/hooks/
$ git config --global core.hooksPath ~/dotfiles/_git/hooks/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Git has added the option in your global Git config:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat ~/.gitconfig
...
[user]
    name = Raphael Brugier
    email = raphael.b_____r@gmail.com
[core]
    hooksPath = ~/dotfiles/_git/templates/hooks/
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;check-email-hook&#34;&gt;Check email hook&lt;/h1&gt;

&lt;p&gt;The goal is now to use a hook to validate the email set in the repository before committing.&lt;/p&gt;

&lt;p&gt;I follow a simple convention to differentiate my personal and professional repositories.
My professional repositories are located on &lt;code&gt;~/devs/ippon&lt;/code&gt; and my personal on &lt;code&gt;~/devs/github/&lt;/code&gt; or &lt;code&gt;~/devs/projects/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Write a bash script to check the email configured based on the repository&amp;rsquo;s path as a pre-commit hook :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vim ~/dotfiles/_git/hooks/pre-commit
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/sh
PWD=`pwd`
if [[ $PWD == *&amp;quot;ippon&amp;quot;* ]] # 1)
then
  EMAIL=$(git config user.email)
  if [[ $EMAIL == *&amp;quot;ippon&amp;quot;* ]] # 2)
  then
    echo &amp;quot;&amp;quot;;
  else
    echo &amp;quot;email not configured to Ippon in Ippon directory&amp;quot;;
    echo &amp;quot;run:&amp;quot;
    echo &#39;   git config user.email &amp;quot;rbrugier@ipponusa.com&amp;quot;&#39;
    echo &#39;&#39;
    exit 1; # 3)
  fi;
fi;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;check if the current path contains &amp;ldquo;ippon&amp;rdquo;&lt;/li&gt;
&lt;li&gt;check if the configured email also contains &amp;ldquo;ippon&amp;rdquo;&lt;/li&gt;
&lt;li&gt;if not, exit the script with an error. This will cancel the commit.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;

&lt;p&gt;Test the hook is working in a new repository:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ mkdir -p ~/devs/ippon/testhook/
$ cd ~/devs/ippon/testhook
$ git init
$ git commit -m &amp;quot;test&amp;quot;
&amp;gt; email not configured to Ippon in Ippon directory
&amp;gt; run:
&amp;gt;   git config user.email &amp;quot;rbrugier@ipponusa.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Git hooks are powerful to automate tests and avoid mistakes. Other common usages of hooks are to trigger tests before a commit or a deployment after a push.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>My development environment</title>
      <link>https://www.raphael-brugier.com/blog/my-development-environment/</link>
      <pubDate>Sun, 27 Nov 2016 18:02:03 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/my-development-environment/</guid>
      <description>&lt;p&gt;A customized development environment could be a huge productivity boost in the day to day work.
In this post, I will share the tools and configurations I currently use.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;terminal-iterm-2&#34;&gt;Terminal: iTerm 2&lt;/h1&gt;

&lt;p&gt;I use &lt;a href=&#34;https://iterm2.com/documentation-shell-integration.html&#34;&gt;iTerm 2&lt;/a&gt; 2 instead of the default terminal on mac.
On linux, I used terminator.&lt;/p&gt;

&lt;p&gt;iTerm 2 adds several features, the most important for me is the ability to have multiple panes and tabs.&lt;/p&gt;

&lt;h1 id=&#34;shell-zsh&#34;&gt;Shell: Zsh&lt;/h1&gt;

&lt;p&gt;Zsh is a shell very similar to bash, but it adds a lot of cooler features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a programmable autocompletion. You can navigate with the arrows keys the completion for example when browsing a directory&lt;/li&gt;

&lt;li&gt;&lt;p&gt;a super history substring search&lt;/p&gt;

&lt;p&gt;brew install zsh&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;oh-my-zsh-and-zgen-to-customize-zsh&#34;&gt;oh-my-zsh and Zgen to customize Zsh&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/robbyrussell/oh-my-zsh&#34;&gt;Oh-my-zsh&lt;/a&gt; is probably the most well known set of plugins to enhance Zsh.
It&amp;rsquo;s a super git repository of plugins, which you can enable to add aliases and parameters completion to your favorites tools.
I have used it for 3 years and have now switched to &lt;a href=&#34;https://github.com/tarjoilija/zgen&#34;&gt;Zgen&lt;/a&gt; to manage my plugins.
Zgen loads the plugins faster than oh-my-zsh and more importantly can load plugins from various github repositories.&lt;/p&gt;

&lt;p&gt;Here is my current list of plugins:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  #from zsh
  zgen oh-my-zsh
  zgen oh-my-zsh plugins/autojump
  zgen oh-my-zsh plugins/colored-man-pages
  zgen oh-my-zsh plugins/docker
  zgen oh-my-zsh plugins/docker-compose
  zgen oh-my-zsh plugins/git
  zgen oh-my-zsh plugins/golang
  zgen oh-my-zsh plugins/sudo

  #theme
  zgen load bhilburn/powerlevel9k powerlevel9k

  #extra plugins
  zgen load djui/alias-tips
  zgen load zsh-users/zsh-syntax-highlighting
  zgen load zsh-users/zsh-history-substring-search
  zgen load zsh-users/zsh-autosuggestions
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;autojump&#34;&gt;autojump&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/wting/autojump&#34;&gt;autojump&lt;/a&gt; is a faster way to navigate the directories, to jump to a directory that contains &amp;ldquo;Foo&amp;rdquo; in its name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  j foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install it with homebrew and then just add the corresponding oh-my-zsh plugin to load it in your $PATH:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install autojump
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;djui-alias-tips&#34;&gt;djui/alias-tips&lt;/h2&gt;

&lt;p&gt;This Zsh plugin will suggest the corresponding shorter alias after you type a command. Very useful when used in combination with one of the the oh-my-zsh plugins which could add a lot of aliases.&lt;/p&gt;

&lt;h1 id=&#34;dotfiles&#34;&gt;dotfiles&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve started versioning my &amp;ldquo;dotfiles&amp;rdquo; configuration a few years ago.
For this I use a git repository in my home: ~/dotfiles
The repository structure follows the hierarchy to install in my home directory.
For each file, a symbolic link is created in my home directory to point to the real file in my dotfile repository.&lt;/p&gt;

&lt;p&gt;The install script replace the &amp;ldquo;_&amp;rdquo; character with a dot:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;!/bin/bash

# adapted from https://github.com/skeeto/dotfiles/blob/master/install.sh

## Install each _-prefixed file or dir
find . -regex &amp;quot;./_.*&amp;quot; -type f -print0 | sort -z | while read -d $&#39;\0&#39; file
do
    dotfile=${file/.\/_/.}

    ## Install directory first
    if [ ! -e $(dirname ~/$dotfile) ]; then
    	echo &amp;quot;create directory: &amp;quot;
    	echo $(dirname ~/$dotfile)
        mkdir -p -m 700  $(dirname ~/$dotfile)
    fi

    ## Create a link to the repository version
    echo Installing $dotfile
    ln -fs $(pwd)/$file ~/$dotfile
    chmod go-rwx $file
done
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;dev-tools&#34;&gt;Dev tools:&lt;/h1&gt;

&lt;p&gt;The other tools I use every day, which would require a post on their own for each&lt;/p&gt;

&lt;h3 id=&#34;intellij&#34;&gt;IntelliJ&lt;/h3&gt;

&lt;p&gt;The best IDE! The refactoring tools are still impressive, and too often unknown.&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;I use Docker to test technologies, start and throw away tools for my projects, etc&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>git commit fixup</title>
      <link>https://www.raphael-brugier.com/blog/git-commit-fixup/</link>
      <pubDate>Wed, 23 Nov 2016 18:37:21 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/git-commit-fixup/</guid>
      <description>&lt;p&gt;In this article, I will describe a git option to quickly fix a previous commit.
This sometimes happens when I want to fix a typo in a previous commit after few new commits.
The goal is to keep a &amp;ldquo;clean&amp;rdquo; git history with consistent commits adding features to facilitate the code reviews.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;(!) Of course, like any &amp;ldquo;rewriting history&amp;rdquo; command which modifies a previous command, it should be used with caution and never used to modify a commit already push.&lt;/p&gt;

&lt;h2 id=&#34;the-options&#34;&gt;The options&lt;/h2&gt;

&lt;p&gt;The option is &lt;code&gt;--fixup&lt;/code&gt; to create a commit fixing a previous one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git commit --fixup &amp;lt;commit to fix&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And to automatically apply the &lt;code&gt;fixup&lt;/code&gt; commit when rebasing, add &lt;code&gt;--autosquash&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git rebase -i origin/master --autosquash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;example&#34;&gt;Example:&lt;/h2&gt;

&lt;p&gt;Starting with the following git history:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$git log --oneline --decorate
d36dc2f code code code
7add401 add README
fb5b59c initial commit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you realize you did a typo in the initial README file you can fix and want to modify the &amp;lsquo;add README&amp;rsquo; commit instead of creating a new commit for a typo:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;${fix the typo}
$git add .
$git commit --fixup 7add401
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Git has created a commit with a message prefixed with &amp;lsquo;!fixup&amp;rsquo; :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$git log --oneline --decorate
7fd8071 (HEAD -&amp;gt; master) fixup! add README
d36dc2f code code code
7add401 add README
fb5b59c initial commit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, you can rewrite the git history of the 3 previous commits with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git rebase --interactive --autosquash HEAD~3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the result:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$git log --oneline --decorate
3ec6daa (HEAD -&amp;gt; master) code code code
d6c4c24 add README
fb5b59c initial commit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;--autosquash&lt;/code&gt; option has automatically reordered and applied the fixup commit.
Note that this option only works with an interactive rebase.&lt;/p&gt;

&lt;p&gt;Since the &lt;code&gt;--autosquash&lt;/code&gt; option only applies to fixup commit, it is safe to  enable it by default in the git config:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git config --global rebase.autosquash true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And voilà! No more almost empty commits to fix typos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://robots.thoughtbot.com/autosquashing-git-commits&#34;&gt;source&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using Docker to simplify Cassandra development in JHipster</title>
      <link>https://www.raphael-brugier.com/blog/using-docker-to-simplify-cassandra-development-in-jhipster/</link>
      <pubDate>Mon, 27 Jun 2016 20:33:11 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/using-docker-to-simplify-cassandra-development-in-jhipster/</guid>
      <description>&lt;p&gt;JHipster is an open source project that generates a fully working application in seconds. With a minimal configuration, JHipster accelerates the start of new projects by integrating frontend, backend, security and a database.&lt;/p&gt;

&lt;p&gt;Cassandra is one of the supported databases and JHipster generates all the configuration needed to access the cluster.&lt;/p&gt;

&lt;p&gt;But it is often hard for the developers to configure and maintain a local Cassandra cluster.&lt;/p&gt;

&lt;p&gt;Moreover, there is no standard tooling to manage the schema migrations, like Liquibase or Flyway for SQL databases, making it difficult to synchronize the schema between every environment and a local configuration.&lt;/p&gt;

&lt;p&gt;JHipster’s goal is to provide the most simple and productive development environment out of the box for the developers, and this tool has been added in the latest (3.4.0) version.&lt;/p&gt;

&lt;p&gt;In this post, I’ll describe the design of the tool and the basic commands to use it.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;design&#34;&gt;Design&lt;/h1&gt;

&lt;p&gt;A Docker Compose[1] configuration is generated to start a Cassandra cluster locally with one command. A set of bash scripts automatically save the executed migrations in a dedicated table. This allows you to automatically[2] execute only the new migration scripts when deploying a new version of the application.
The tool can be used both by the developers to synchronize their local cluster and in production to keep track of the schema migrations.&lt;/p&gt;

&lt;p&gt;The standardization of the migration scripts is also used by the integration tests to start an in-memory Cassandra cluster.&lt;/p&gt;

&lt;p&gt;[1]: JHipster uses the v2 of the Docker Compose file format and Docker 1.10+ and Compose 1.6+ are required.&lt;/p&gt;

&lt;p&gt;[2]: Because the migration scripts are read from a Docker volume, the sources must be located in the use directory.&lt;/p&gt;

&lt;h2 id=&#34;basic-commands-to-run-the-migration-tool&#34;&gt;Basic commands to run the migration tool&lt;/h2&gt;

&lt;p&gt;Launch a JHipster application and a Cassandra cluster:&lt;/p&gt;

&lt;p&gt;First you need to build a Docker image containing your application:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mvnw package -Pprod docker:build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you can run this image and the other services with Docker Compose:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f src/main/docker/app.yml up -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compose starts 4 Docker containers:&lt;/p&gt;

&lt;p&gt;The application image
- A first Cassandra node acting as the contact point
- A second node joining the cluster
- A service executing the schema migrations
- Thanks to Docker Compose, it is easy to add new nodes in the cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f src/main/docker/app.yml scale yourapp-cassandra-node=2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The new node automatically joins the Cassandra cluster using the first node as a contact point.
The container executing the migrations reads a dedicated folder – &lt;code&gt;config/cql/changelogs/&lt;/code&gt; by convention – to find the migration scripts.&lt;/p&gt;

&lt;p&gt;Like Liquibase, the migration tool stores the metadata of the executed scripts in a table named schema_version to keep track of executed scripts.&lt;/p&gt;

&lt;h2 id=&#34;modifying-the-schema-with-jhipster-and-using-the-migration-tool&#34;&gt;Modifying the schema with JHipster and using the migration tool:&lt;/h2&gt;

&lt;p&gt;With JHipster, it is possible to create a new domain entity in one command with a few questions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ yo jhipster:entity book
Generating field #1? Do you want to add a field to your entity? Yes
? What is the name of your field? title

? What is the type of your field? String

? Do you want to add validation rules to your field? No
Generating field #2

? Do you want to add a field to your entity? Yes

? What is the name of your field? author

? What is the type of your field? String

? Do you want to add validation rules to your field? No
Generating field #3

? Do you want to add a field to your entity? Yes

? What is the name of your field? releaseDate

? What is the type of your field? LocalDate (Warning: only compatible with cassandra v3)

? Do you want to add validation rules to your field? No
? Do you want to use a Data Transfer Object (DTO)? No, use the entity directly

? Do you want to use separate service class for your business logic? No, the REST controller should use the repository directly
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JHipster generates AngularJS code for the frontend and java code for the basic CRUD operations. It also generates the CQL script to create the Cassandra table for this new entity:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE TABLE IF NOT EXISTS book (
   id uuid,
   title text,
   author text,
   releaseDate date,
   PRIMARY KEY(id)
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without stopping the cluster, you can execute the migration tool to run the CQL script:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f src/main/docker/app.yml up yourapp-cassandra-migration
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Package the application into a new image and relaunch only its container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mvnw package -Pprod docker:build
docker-compose -f src/main/docker/app.yml up -d --no-deps yourapp-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without restarting the Cassandra cluster, JHipster has created all the screens, the java code and has executed the migration script to create the new Cassandra table:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://www.raphael-brugier.com/img/jhipster-default-books.png&#34; width=&#34;100%&#34; /&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;By providing a simple tool to manage a Cassandra environment for development, tests and deployment JHipster is also providing best practices to start new applications based on Cassandra.&lt;/p&gt;

&lt;p&gt;You can find more on the &lt;a href=&#34;http://jhipster.github.io/&#34;&gt;JHipster project website&lt;/a&gt;.
Ippon USA is hosting a master class on JHipster with material designed by the JHipster creator, &lt;a href=&#34;https://www.eventbrite.com/e/jhipster-master-class-ippon-usa-tickets-21358779685&#34;&gt;register here&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A tour of Databricks Community Edition: a hosted Spark service</title>
      <link>https://www.raphael-brugier.com/blog/a-tour-of-databricks-community-edition-a-hosted-spark-service/</link>
      <pubDate>Wed, 13 Apr 2016 19:51:51 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/a-tour-of-databricks-community-edition-a-hosted-spark-service/</guid>
      <description>&lt;p&gt;With the &lt;a href=&#34;https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html&#34;&gt;recent announcement&lt;/a&gt; of the Community Edition, it’s time to have a look at the Databricks Cloud solution.
Databricks Cloud is a hosted Spark service from Databricks, the team behind Spark.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Databricks Cloud offers many features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A cluster management service.

&lt;ul&gt;
&lt;li&gt;The service will spin up Amazon EC2 instances with Spark nodes already set up for you.&lt;/li&gt;
&lt;li&gt;Free 6GB memory cluster for the Community Edition and billed hourly per node for the regular version.&lt;/li&gt;
&lt;li&gt;The price will depend on the size of the instances and you can even mix on-demand and spot instances.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A notebook, to write Spark code either in Scala, Python or R, with version control and user role management.&lt;/li&gt;
&lt;li&gt;A scheduling service to turn notebooks or fat JARs into managed jobs.

&lt;ul&gt;
&lt;li&gt;The service also allows to manage streaming jobs and have failure notifications, as well as auto restart capabilities.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;And &lt;a href=&#34;https://databricks.com/blog/2016/02/17/introducing-databricks-dashboards.html&#34;&gt;more recently&lt;/a&gt;, a dashboarding service to turn your notebooks snippets into custom dashboard components.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The notebook is where you will spend most of your time. It offers a fully interactive Spark environment, with the capabilities to add any dependencies from the Maven Central repository or to upload your own JARs to the cluster.&lt;/p&gt;

&lt;p&gt;Notebooks have been used for years for data exploration, but with the rise of Data Science, there has been a lot of traction for tools such as &lt;a href=&#34;http://jupyter.org/&#34;&gt;Jupyter&lt;/a&gt;, &lt;a href=&#34;http://spark-notebook.io/&#34;&gt;Spark notebook&lt;/a&gt;, or &lt;a href=&#34;https://zeppelin.incubator.apache.org/&#34;&gt;Apache Zeppelin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jupyter is an historical Python notebook (formerly known as IPython) that have been added a Spark extension, you could use Python of course, but also R and Scala. It’s more mature than Zeppelin and Jupyter notebooks are even integrated in &lt;a href=&#34;https://github.com/blog/1995-github-jupyter-notebooks-3&#34;&gt;GitHub&lt;/a&gt;.
But it would requires some extra configuration to get Scala and Spark support.&lt;/p&gt;

&lt;p&gt;Spark notebook was one of the first notebook to appear for Spark. It is limited to the Scala language, so it might not be the best choice if you have data analysts working primarily with Python.&lt;/p&gt;

&lt;p&gt;Zeppelin is still an incubating project from the Apache Foundation but it has received a lot of traction lately and it is promising. Compared to Databricks Cloud’s built-in notebook, Zeppelin is not dedicated to Spark but supports many more technologies via various connectors such as Cassandra or Flink. You will of course have to manage the deployment and configuration by yourself, but with the main benefit of having a fined-grained control over the infrastructure. While the Community Edition of Databricks Cloud involves some restrictions – smaller Amazon EC2 instances and no access to the scheduling component – it is still a great tool to get started with Spark, especially for learning and fast prototyping.&lt;/p&gt;

&lt;p&gt;To complete this introduction, let’s write an example of a Twitter stream processing and some visualizations.&lt;/p&gt;

&lt;p&gt;In this example, we’ll subscribe to the Twitter stream API which delivers roughly a 1% sample of all the tweets published in realtime. We’ll use Spark Streaming to process the stream and identify the language and country of each tweet.
We will store a sliding window of the results as a table and display the results as built-in visualizations in the notebook.&lt;/p&gt;

&lt;h1 id=&#34;step-0-community-edition-access&#34;&gt;Step 0: Community Edition access&lt;/h1&gt;

&lt;p&gt;You first need to &lt;a href=&#34;http://go.databricks.com/databricks-community-edition-beta-waitlist&#34;&gt;subscribe&lt;/a&gt; to Databricks Community Edition. This is still a private beta version but you should receive your invitation within one week.&lt;/p&gt;

&lt;p&gt;Once you have the Databricks Cloud, &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7631468844903857/1641217975453210/8780643444584178/latest.html&#34;&gt;import my notebook&lt;/a&gt;. This notebook is a partial reuse of the Databricks &lt;a href=&#34;https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#08%20Spark%20Streaming/03%20Twitter%20Hashtag%20Count%20-%20Scala.html&#34;&gt;Twitter hash count&lt;/a&gt; example.&lt;/p&gt;

&lt;h1 id=&#34;step-1-prerequisite-libraries-and-imports&#34;&gt;Step 1: prerequisite libraries and imports&lt;/h1&gt;

&lt;p&gt;The example uses the Apache Tika library for the language recognition of the tweets.
To attach the dependency to your Spark cluster, follow these steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the workspace, in your user space, open the “Create” dialog box and choose “library”&lt;/li&gt;
&lt;li&gt;Choose “maven coordinate” as a source&lt;/li&gt;
&lt;li&gt;Use “org.apache.tika:tika-core:1.12” as the Coordinate&lt;/li&gt;
&lt;li&gt;Make sure the “Attach automatically to all clusters.” box is checked in the library details of your workspace.&lt;/li&gt;
&lt;li&gt;The library and its dependencies will now be deployed to the cluster nodes.&lt;/li&gt;
&lt;li&gt;To verify this, you can access the “Cluster” tab and see “1 library loaded” in the your cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/clusterView.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;step-2-twitter-credentials&#34;&gt;Step 2: Twitter credentials&lt;/h1&gt;

&lt;p&gt;Because this example requires a connection to the Twitter stream API, you should create a Twitter application and acquire an OAuth token.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://apps.twitter.com/&#34;&gt;https://apps.twitter.com/&lt;/a&gt; and follow the steps to create your Twitter application.&lt;/li&gt;
&lt;li&gt;You should then answer Step 2 questions to enter your credentials.&lt;/li&gt;
&lt;li&gt;These credentials will then be automatically picked by the Twitter4j library and the Spark Streaming wrapper to create a Twitter stream.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/twitterCredentials.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;step-3-run-the-twitter-streaming-job&#34;&gt;Step 3: Run the Twitter streaming job&lt;/h1&gt;

&lt;p&gt;Execute step 3’s code in the notebook, so as to create a StreamingContext and run it in the cluster.&lt;/p&gt;

&lt;p&gt;The code will initialize the Twitter stream, and for each tweet received, it will:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Transform the country code from a two-letter code to a three-letter code. This is because the Databricks notebook requires an ISO 3166-1 alpha-3 code for the country.&lt;/li&gt;
&lt;li&gt;Detect the language of the tweet using the Tika library. Because tweets are small portions of text containing hashtags, usernames, etc; the detection could unfortunately be inaccurate.&lt;/li&gt;
&lt;li&gt;Wrap the result in a Tweet case class.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output of the stream, a sliding window of the last 30 seconds tweets, is then written to a temporary “SQL” table, to be queryable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Tweet(user: String, text: String, countryCode: String, language: String)

// Initialize the language identifier library
LanguageIdentifier.initProfiles()

// Initialize a map to convert Countries from 2 chars iso encoding to 3 characters
val iso2toIso3Map: Map[String, String] = Locale.getISOCountries()
  .map(iso2 =&amp;gt; iso2 -&amp;gt; new Locale(&amp;quot;&amp;quot;, iso2).getISO3Country)
  .toMap

// detect a language from a text content using the Apache Tika library
def detectLanguage(text: String): String = {
    new LanguageIdentifier(text).getLanguage
}

// This is the function that creates the SteamingContext and sets up the Spark Streaming job.
def creatingFunc(): StreamingContext = {
  // Create a Spark Streaming Context.
  val slideInterval = Seconds(1)
  val ssc = new StreamingContext(sc, slideInterval)
  ssc.remember(Duration(100))
  // Create a Twitter Stream for the input source. 
  val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))

  val twitterStream = TwitterUtils.createStream(ssc, auth)
          .filter(t=&amp;gt; t.getPlace != null)
          .map(t =&amp;gt; Tweet(t.getUser.getName, t.getText, iso2toIso3Map.getOrElse(t.getPlace.getCountryCode, &amp;quot;&amp;quot;), detectLanguage(t.getText)))
                .window(windowDuration = Seconds(30), slideDuration = Seconds(10))
            .foreachRDD { rdd =&amp;gt; 
              val sqlContext = SQLContext.getOrCreate(SparkContext.getOrCreate())
                  // this is used to implicitly convert an RDD to a DataFrame.
                import sqlContext.implicits._
                rdd.toDF().registerTempTable(&amp;quot;tweets&amp;quot;)
            }
  ssc
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;step-4-visualizations&#34;&gt;Step 4: Visualizations&lt;/h1&gt;

&lt;p&gt;Now, tweets are automatically stored and updated from the sliding window and we can query the table and use the notebook’s built-in visualizations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweets by country:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/tweetsByCountry.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tweets by language:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/tweetsByLanguage.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can run virtually any SQL query on the last 30 seconds of the 1% sample of tweets emitted from all-over the world!&lt;/p&gt;

&lt;p&gt;Even if the visualizations can be exported to a dashboard, they still need to be refreshed manually. This is because you cannot create Spark jobs in the community edition. However, the non-Community version allows to &lt;a href=&#34;https://community.cloud.databricks.com/?o=7631468844903857#externalnotebook/https%3A%2F%2Fdocs.cloud.databricks.com%2Fdocs%2Flatest%2Fdatabricks_guide%2Findex.html%2302%2520Product%2520Overview%2F06%2520Jobs.html&#34;&gt;turn this notebook into an actual Spark Streaming job&lt;/a&gt; running indefinitely while refreshing a dashboard of visualizations.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Databricks Community Edition offers a nice subset of Databricks Cloud for free. It is a nice playground to start with Spark and notebooks. It also offers the integration of the very complete &lt;a href=&#34;https://community.cloud.databricks.com/?o=7631468844903857#externalnotebook/https%3A%2F%2Fdocs.cloud.databricks.com%2Fdocs%2Flatest%2Fcourses%2Findex.html%23Introduction%2520to%2520Big%2520Data%2520with%2520Apache%2520Spark%2520(CS100-1x)%2FIntroduction%2520(README).html&#34;&gt;Introduction to Big Data with Apache Spark&lt;/a&gt; course taught by Berkeley University.&lt;/p&gt;

&lt;p&gt;Besides this, before jumping to the professional edition, you will have to consider the tradeoffs between an all-in-one service like Databricks Cloud – that can become pricey for long running jobs – versus managed clusters (Amazon EMR, Google Dataproc, …) or in-house hosting with fine grained control of the infrastructure of the nodes but with additional maintenance costs.&lt;/p&gt;

&lt;p&gt;See the notebook in action in &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7631468844903857/1641217975453210/8780643444584178/latest.html&#34;&gt;Databricks cloud&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Testing strategy for Spark Streaming – Part 2 of 2</title>
      <link>https://www.raphael-brugier.com/blog/testing-strategy-for-apache-spark-jobs-2-of-2/</link>
      <pubDate>Wed, 30 Mar 2016 19:25:01 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/testing-strategy-for-apache-spark-jobs-2-of-2/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://www.raphael-brugier.com/blog/testing-strategy-for-apache-spark-jobs-1-of-2/&#34;&gt;previous post&lt;/a&gt;, we’ve seen why it’s important to test your Spark jobs and how you could easily unit test the job’s logic, first by designing your code to be testable and then by writing unit tests.&lt;/p&gt;

&lt;p&gt;In this post, we will look at applying the same pattern to another important part of the Spark engine: Spark Streaming.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;spark-streaming-example&#34;&gt;Spark Streaming example&lt;/h1&gt;

&lt;p&gt;Spark Streaming is an extension of Spark to implement streaming processing on top of the batch engine. The base idea is to accumulate a flow of events in micro-batches and then process them separately. As for a signal, the stream is discretized and thus named DStream in the Spark API.&lt;/p&gt;

&lt;p&gt;For this example, we will simply generate a stream of characters in input, and have each character capitalized in the output stream. To spice up the thing, the output will be a sliding window.&lt;/p&gt;

&lt;p&gt;We will configure the batch duration to 1 second, the window duration to 3 seconds and the slide duration to 2 seconds.&lt;/p&gt;

&lt;p&gt;This is better explained with the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.raphael-brugier.com/img/sparkCapitalizedWindowedStream.png&#34; alt=&#34;Spark capitalized windowed stream&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And the code implementing this process:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.{Seconds, StreamingContext}
import scala.collection.mutable

object MainStreaming {

  val sparkConf = new SparkConf()
    .setMaster(&amp;quot;local[*]&amp;quot;)
    .setAppName(&amp;quot;spark-streaming-testing-example&amp;quot;)

  val ssc = new StreamingContext(sparkConf, Seconds(1))

  def main(args: Array[String]) {

    val rddQueue = new mutable.Queue[RDD[Char]]()

    ssc.queueStream(rddQueue)
      .map(_.toUpper)
      .window(windowDuration = Seconds(3), slideDuration = Seconds(2))
      .print()

    ssc.start()

    for (c &amp;lt;- &#39;a&#39; to &#39;z&#39;) {
      rddQueue += ssc.sparkContext.parallelize(List(c))
    }

    ssc.awaitTermination()
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, we simulate a stream of characters by continuously adding them as RDDs of one character in a mutable Queue.&lt;/p&gt;

&lt;p&gt;We create a QueueStream to wrap the Queue as an InputDStream. Because the QueueInputDStream is mainly designed for testing, each RDD is consumed one by one by default. We have configured the StreamingContext to have batches of 1 second, hence each character will be consumed by the stream logic every 1 second.&lt;/p&gt;

&lt;p&gt;When you run this code, the Spark engine will print the last 3 capitalized letters every 2 seconds.&lt;/p&gt;

&lt;h1 id=&#34;testing-stream-operations&#34;&gt;Testing stream operations&lt;/h1&gt;

&lt;p&gt;As in the previous article, the pattern to make the code testable consists in extracting the logic in a separate function that takes a DStream in parameter and that returns the resulting DStream.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.dstream.DStream

object StreamOperations {

  def capitalizeWindowed(input: DStream[Char]): DStream[Char] = {
    input.map(_.toUpper)
          .window(windowDuration = Seconds(3), slideDuration = Seconds(2))
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main problem with testing Streams is their time-based nature. To compare the output stream to an expected other set of data, you will have to do the assertion at the instant where the engine consumes the input. Adding &lt;code&gt;Thread.Sleep(...)&lt;/code&gt; would be an inaccurate solution and will have the major drawback to dramatically slow down your tests.&lt;/p&gt;

&lt;p&gt;The appropriate solution to control the time is to use the “Virtual Clock” pattern, where you replace the system’s clock with your own implementation. During the execution of the test, you can then change the value returned by the clock and therefore control the timing.&lt;/p&gt;

&lt;p&gt;Internally, Spark uses this pattern with a &lt;a href=&#34;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Clock.scala#L23&#34;&gt;Clock&lt;/a&gt; interface and a default implementation returning the system time. It allows to replace this implementation by your own implementation by setting the &lt;code&gt;spark.streaming.clock&lt;/code&gt; param when configuring the &lt;code&gt;SparkContext&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is how Spark internal unit tests work, by replacing the &lt;a href=&#34;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Clock.scala#L31&#34;&gt;SystemClock&lt;/a&gt; implementation with &lt;a href=&#34;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ManualClock.scala#L27&#34;&gt;ManualClock&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately, the Clock interface has a package protected visibility limited to the &lt;code&gt;org.apache.spark package&lt;/code&gt;. But we can workaround this by placing our own implementation extending the interface in this same package.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package org.apache.spark
import java.util.Date
import org.apache.spark.streaming.Duration

class FixedClock(var currentTime: Long) extends org.apache.spark.util.Clock {

  def this() = this(0L)

  def setCurrentTime(time: Date): Unit = synchronized {
    currentTime = time.getTime
    notifyAll()
  }

  def addTime(duration: Duration): Unit = synchronized {
    currentTime += duration.toMillis
    notifyAll()
  }

  override def getTimeMillis(): Long = synchronized {
    currentTime
  }

  override def waitTillTime(targetTime: Long): Long = synchronized {
    while (currentTime &amp;lt; targetTime) {
      wait(10)
    }
    getTimeMillis()
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then add an utility method to workaround the private accessibility of the &lt;code&gt;Clock&lt;/code&gt; in the &lt;code&gt;StreamingContext&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package org.apache.spark.streaming
import org.apache.spark.FixedClock

object Clock {
  def getFixedClock(ssc: StreamingContext): FixedClock = {
    ssc.scheduler.clock.asInstanceOf[FixedClock]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test can now be written:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import java.util.concurrent.TimeUnit
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.{Clock, Seconds, StreamingContext}
import org.apache.spark.{FixedClock, SparkConf, SparkContext}
import org.scalatest.concurrent.Eventually
import org.scalatest.time.{Millis, Span}
import org.scalatest.{BeforeAndAfter, FlatSpec, Matchers}
import scala.collection.mutable
import scala.collection.mutable.ListBuffer
import scala.concurrent.duration.Duration

class StreamingTest extends FlatSpec with Matchers with BeforeAndAfter with Eventually {

  var sc:SparkContext = _
  var ssc: StreamingContext = _
  var fixedClock: FixedClock = _

  override implicit val patienceConfig = PatienceConfig(timeout = scaled(Span(1500, Millis)))

  before {
    val sparkConf = new SparkConf()
      .setMaster(&amp;quot;local[*]&amp;quot;)
      .setAppName(&amp;quot;test-streaming&amp;quot;)
      .set(&amp;quot;spark.streaming.clock&amp;quot;, &amp;quot;org.apache.spark.FixedClock&amp;quot;)

    ssc = new StreamingContext(sparkConf, Seconds(1))
    sc = ssc.sparkContext
    fixedClock = Clock.getFixedClock(ssc)
  }

  after {
    ssc.stop(stopSparkContext = true, stopGracefully = false)
  }

  behavior of &amp;quot;stream transformation&amp;quot;

  it should &amp;quot;apply transformation&amp;quot; in {
    val inputData: mutable.Queue[RDD[Char]] = mutable.Queue()
    var outputCollector = ListBuffer.empty[Array[Char]]

    val inputStream = ssc.queueStream(inputData)
    val outputStream = StreamOperations.capitalizeWindowed(inputStream)

    outputStream.foreachRDD(rdd=&amp;gt; {outputCollector += rdd.collect()})

    ssc.start()

    inputData += sc.makeRDD(List(&#39;a&#39;))
    wait1sec() // T = 1s

    inputData += sc.makeRDD(List(&#39;b&#39;))
    wait1sec() // T = 2s

    assertOutput(outputCollector, List(&#39;A&#39;,&#39;B&#39;))

    inputData += sc.makeRDD(List(&#39;c&#39;))
    wait1sec() // T = 3s

    inputData += sc.makeRDD(List(&#39;d&#39;))
    wait1sec() // T = 4s
    assertOutput(outputCollector, List(&#39;B&#39;, &#39;C&#39;, &#39;D&#39;))

    // wait until next slide
    wait1sec() // T = 5s
    wait1sec() // T = 6s
    assertOutput(outputCollector, List(&#39;D&#39;))
  }

  def assertOutput(result: Iterable[Array[Char]], expected: List[Char]) =
    eventually {
      result.last.toSet should equal(expected.toSet)
    }

  def wait1sec(): Unit = {
    fixedClock.addTime(Seconds(1))
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here again, we start a StreamingContext before running the test and we’ll stop it after.
We also replace the clock with our own implementation so that we can control the time.
Like in the &lt;code&gt;Main&lt;/code&gt; example, we use a &lt;code&gt;QueueInputDStream&lt;/code&gt; to simulate the input of data.
We also use a &lt;code&gt;ListBuffer&lt;/code&gt; to stack the resulting value of the &lt;code&gt;OuputDStream&lt;/code&gt; collected with the &lt;code&gt;forEachRDD(…)&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;One extra tricky thing…&lt;/p&gt;

&lt;p&gt;Because the processing of the DStream occurs in a separated thread, when we change the time to the exact instant of the sliding window trigger, it still requires a few extra ms to do the actual computation.
Thus, as suggested in &lt;a href=&#34;http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing/&#34;&gt;this post&lt;/a&gt; about testing Spark, we protect the assertion with an eventually block from ScalaTest’s Eventually trait.
The patienceConfig set the timeout to retry the assertion multiple times before a time-out.&lt;/p&gt;

&lt;p&gt;We now have a fast and predictable test to ensure the streaming operations’ correctness!&lt;/p&gt;

&lt;h1 id=&#34;spark-testing-base-library&#34;&gt;Spark-testing-base library&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/holdenk/spark-testing-base&#34;&gt;Spark-testing-base&lt;/a&gt; library have some built-in trait and methods to help you test the streaming logic.&lt;/p&gt;

&lt;p&gt;When using the library, the &lt;code&gt;FixedClock&lt;/code&gt; workaround is also already implemented.&lt;/p&gt;

&lt;p&gt;Let’s rewrite the window test with the StreamingSuiteBase trait:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import com.holdenkarau.spark.testing.StreamingSuiteBase

class StreamingWithSparkTestingTest extends StreamingSuiteBase {

  test(&amp;quot;capitalize by window&amp;quot;) {
    val input = List(List(&#39;a&#39;), List(&#39;b&#39;), List(&#39;c&#39;), List(&#39;d&#39;), List(&#39;e&#39;))

    val slide1 = List(&#39;A&#39;, &#39;B&#39;)
    val slide2 = List(&#39;B&#39;, &#39;C&#39;, &#39;D&#39;)
    val expected = List(slide1, slide2)

    testOperation(input, StreamOperations.capitalizeWindowed, expected)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;StreamingSuiteBase&lt;/code&gt; trait will take care of all the setup we had manually prepared in the previous test: starting and stopping the &lt;code&gt;StreamingContext&lt;/code&gt;, replacing the &lt;code&gt;Clock&lt;/code&gt; in the engine, and incrementing the time when each element is consumed.&lt;/p&gt;

&lt;p&gt;Unlike the &lt;code&gt;RDDComparisons.compare(...)&lt;/code&gt; method shown in the previous article, where the comparison was distributed, the elements are collected locally from the OutputDStream. Be sure not to produce too much data in your tests or you can rapidly run out of memory.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;While requiring more setup and care than testing batch jobs, it is also possible to test Spark Streaming jobs.
Controlling the time is the key to having a predictable output to compare results to expected ones.&lt;/p&gt;

&lt;p&gt;The Spark-testing-base library can be very helpful, especially to avoid the workarounds.&lt;/p&gt;

&lt;p&gt;You can find the code of these examples on &lt;a href=&#34;https://github.com/raphaelbrugier/spark-testing-example&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Testing strategy for Apache Spark jobs – Part 1 of 2</title>
      <link>https://www.raphael-brugier.com/blog/testing-strategy-for-apache-spark-jobs-1-of-2/</link>
      <pubDate>Sat, 12 Mar 2016 18:34:26 -0500</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/testing-strategy-for-apache-spark-jobs-1-of-2/</guid>
      <description>&lt;p&gt;Like any other application, Apache Spark jobs deserve good testing practices and coverage.&lt;/p&gt;

&lt;p&gt;Indeed, the costs of running jobs with production data makes unit testing a must-do to have a fast feedback loop and discover the errors earlier.&lt;/p&gt;

&lt;p&gt;But because of its distributed nature and the RDD abstraction on top of the data, Spark requires special care for testing.&lt;/p&gt;

&lt;p&gt;In this post, we’ll explore how to design your code for testing, how to setup a simple unit-test for your job logic and how the &lt;a href=&#34;https://github.com/holdenk/spark-testing-base&#34;&gt;spark-testing-base&lt;/a&gt; library can help.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;design-for-testing&#34;&gt;Design for testing&lt;/h1&gt;

&lt;p&gt;From a higher point of view, any Spark job can be described as an “immutable” a transformation of distributed data.&lt;/p&gt;

&lt;p&gt;In particular, any Spark job can be refactored to a composition of functions taking data as input, the so-called RDD, and returning data, hence a RDD again.&lt;/p&gt;

&lt;p&gt;Extracting the logic of the job into functions will make it possible to reuse the functions across different jobs and to isolate the behavior to test it in a deterministic environment.&lt;/p&gt;

&lt;p&gt;To separate the logic from the scheduling and configuration of the job, you will also want to isolate the logic to a separated object.&lt;/p&gt;

&lt;p&gt;Let’s apply this pattern to the well-known word count example.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import org.apache.spark.{SparkConf, SparkContext}

object Main {

  val sparkConf = new SparkConf()
    .setMaster(&amp;quot;local[*]&amp;quot;)
    .setAppName(&amp;quot;spark-testing-example&amp;quot;)
  val sc = new SparkContext(sparkConf)

  def main(args: Array[String]) {
    val countByWordRdd =  sc.textFile(&amp;quot;src/main/resources/intro.txt&amp;quot;)
      .flatMap(l =&amp;gt; l.split(&amp;quot;\\W+&amp;quot;))
      .map(word =&amp;gt; (word, 1))
      .reduceByKey(_ + _)

    countByWordRdd.foreach(println)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extracting a method is a &lt;a href=&#34;http://refactoring.com/catalog/extractMethod.html&#34;&gt;classic refactoring pattern&lt;/a&gt;. Therefore, this can be easily done with a few keystrokes within your favorite IDE:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Extract the input data in a separated variable to separate it from the logic&lt;/li&gt;
&lt;li&gt;Extract the logic in a count method (select + refactor -&amp;gt; extract -&amp;gt; method)&lt;/li&gt;
&lt;li&gt;Move the method to a new object, WordCounter&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Main {

  val sparkConf = new SparkConf()
    .setMaster(&amp;quot;local[*]&amp;quot;)
    .setAppName(&amp;quot;spark-testing-example&amp;quot;)
  val sc = new SparkContext(sparkConf)

  def main(args: Array[String]) {
    val input: RDD[String] = sc.textFile(&amp;quot;src/main/resources/intro.txt&amp;quot;)
    val countByWordRdd: RDD[(String, Int)] = WordCounter.count(input)

    countByWordRdd.foreach(println)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import org.apache.spark.rdd.RDD

object WordCounter {
  def count(lines: RDD[String]): RDD[(String, Int)] = {
    val wordsCount = lines.flatMap(l =&amp;gt; l.split(&amp;quot;\\W+&amp;quot;))
      .map(word =&amp;gt; (word, 1))
      .reduceByKey(_ + _)
    wordsCount
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;basic-test&#34;&gt;Basic test&lt;/h1&gt;

&lt;p&gt;Now that we have extracted the logic, we can write a test assuming an input data and asserting the result of the function to an expected data. We will use &lt;a href=&#34;http://www.scalatest.org/&#34;&gt;ScalaTest&lt;/a&gt; as a testing framework.&lt;/p&gt;

&lt;p&gt;The tricky part when writing tests for Spark is the RDD abstraction. Your first idea would probably be to mock the input and the expected. But then, you will not be able to execute the Spark behavior on the RDD passed to the function.&lt;/p&gt;

&lt;p&gt;Instead, we have to start a &lt;code&gt;SparkContext&lt;/code&gt; to build the input and expected RDDs and run the transformation in a real Spark environment. Indeed, creating a &lt;code&gt;SparkContext&lt;/code&gt; for unit testing is the &lt;a href=&#34;http://spark.apache.org/docs/latest/programming-guide.html#unit-testing&#34;&gt;recommended approach&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Because starting a &lt;code&gt;SparkContext&lt;/code&gt; is time-consuming, you will save a lot of time starting the context only once before all the tests. Also, even if it possible with some tweaking, it is not recommended to have more than one &lt;code&gt;SparkContext&lt;/code&gt; living in the JVM. So make sure you stop the context after running all the tests and to disable the parallel execution.&lt;/p&gt;

&lt;p&gt;Starting and stopping the &lt;code&gt;SparkContext&lt;/code&gt; can easily be done with the &lt;code&gt;BeforeAndAfter&lt;/code&gt; trait.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}
import org.scalatest.{BeforeAndAfter, FlatSpec, Matchers}

class WordCounterTest extends FlatSpec with Matchers with BeforeAndAfter {

  var sc:SparkContext = _

  before {
    val sparkConf = new SparkConf()
      .setMaster(&amp;quot;local[*]&amp;quot;)
      .setAppName(&amp;quot;test-wordcount&amp;quot;)
    sc = new SparkContext(sparkConf)
  }

  after {
    sc.stop()
  }

  behavior of &amp;quot;Words counter&amp;quot;

  it should &amp;quot;count words in a text&amp;quot; in {
    val text =
      &amp;quot;&amp;quot;&amp;quot;Hello Spark
        |Hello world
      &amp;quot;&amp;quot;&amp;quot;.stripMargin
    val lines: RDD[String] = sc.parallelize(List(text))
    val wordCounts: RDD[(String, Int)] = WordCounter.count(lines)

    wordCounts.collect() should contain allOf ((&amp;quot;Hello&amp;quot;, 2), (&amp;quot;Spark&amp;quot;, 1), (&amp;quot;world&amp;quot;, 1))
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;spark-testing-base-library&#34;&gt;Spark-testing-base library&lt;/h1&gt;

&lt;p&gt;Setting up the &lt;code&gt;before&lt;/code&gt; and &lt;code&gt;after&lt;/code&gt; methods for all your test cases can become tedious if you have many tests. An alternative could be to hold the Context in a Singleton Object and start it once for all the tests, or to inherits a common trait to implement this behavior.&lt;/p&gt;

&lt;p&gt;Also, the previous example works fine when working with a local cluster where all the data can fit in memory.
But if you are testing with a lot of data, a large sample of your production data for example, calling the &lt;code&gt;collect()&lt;/code&gt; method to gather all the data locally to compare with an expected output is no longer an option.&lt;/p&gt;

&lt;p&gt;Fortunately, the spark-testing-base library provides traits and methods to prepare your tests and run distributed comparisons.&lt;/p&gt;

&lt;p&gt;Let’s import the library and rewrite the test:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;pom.xml&lt;/code&gt; extract:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependencies&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-core_${scala.dep.version}&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${spark.version}&amp;lt;/version&amp;gt;
  &amp;lt;/dependency&amp;gt;
  &amp;lt;!--spark-testing has a dependency to spark-sql, spark-hive, spark-mllib --&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-sql_${scala.dep.version}&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${spark.version}&amp;lt;/version&amp;gt;
  &amp;lt;/dependency&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-hive_${scala.dep.version}&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${spark.version}&amp;lt;/version&amp;gt;
  &amp;lt;/dependency&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-mllib_${scala.dep.version}&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${spark.version}&amp;lt;/version&amp;gt;
  &amp;lt;/dependency&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.scalatest&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;scalatest_${scala.dep.version}&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.2.6&amp;lt;/version&amp;gt;
    &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
  &amp;lt;/dependency&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;com.holdenkarau&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spark-testing-base_${scala.dep.version}&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${spark.version}_0.3.2-SNAPSHOT&amp;lt;/version&amp;gt;
  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
  &amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;package com.ipponusa
import com.holdenkarau.spark.testing.{RDDComparisons, RDDGenerator, SharedSparkContext}
import org.apache.spark.rdd.RDD
import org.scalacheck.Arbitrary
import org.scalacheck.Prop._
import org.scalatest.prop.Checkers
import org.scalatest.{FlatSpec, Matchers}

@RunWith(classOf[JUnitRunner])
class WordCounterWithSparkTestingTest extends FlatSpec with Matchers with SharedSparkContext{

  behavior of &amp;quot;Words counter&amp;quot;

  it should &amp;quot;count words in a text&amp;quot; in {
    val text =
      &amp;quot;&amp;quot;&amp;quot;Hello Spark
        |Hello world
      &amp;quot;&amp;quot;&amp;quot;.stripMargin

    val inputRdd: RDD[String] = sc.parallelize(List(text))
    val expectedRdd: RDD[(String, Int)] = sc.parallelize(List((&amp;quot;Hello&amp;quot;, 2), (&amp;quot;Spark&amp;quot;, 1), (&amp;quot;world&amp;quot;, 1)))

    val resRdd: RDD[(String, Int)] = WordCounter.count(inputRdd)
    assert(None === RDDComparisons.compare(resRdd, expectedRdd))
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test class now extends the &lt;code&gt;SharedSparkContext&lt;/code&gt; trait instead of &lt;code&gt;BeforeAndAfter&lt;/code&gt;. This trait will automatically take care of starting and stopping a &lt;code&gt;SparkContext&lt;/code&gt; for you.&lt;/p&gt;

&lt;p&gt;The method RDDComparisons.compare(…) is more interesting.&lt;/p&gt;

&lt;p&gt;Instead of locally collecting the data to be compared, the comparison will be run as RDD operations on Spark nodes. Even if this may involve a lot of shuffling operations, the data is still distributed and thus can fit in memory.&lt;/p&gt;

&lt;p&gt;Of course, in the same manner, the input and expected data would not be loaded locally but most probably from external distributed storage.&lt;/p&gt;

&lt;p&gt;Like HDFS for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val inputRdd = sc.textFile(&amp;quot;hdfs://127.0.0.1:9000/data/test/bigInput.txt&amp;quot;)
val expectedRdd = sc.textFile(&amp;quot;hdfs://127.0.0.1:9000/data/test/bigExpected.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The spark-testing-base library also provides methods for property-based testing via an integration of the &lt;a href=&#34;https://www.scalacheck.org/&#34;&gt;ScalaCheck&lt;/a&gt; library.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class WordCounterWithSparkTestingTest extends FlatSpec with Matchers with SharedSparkContext with Checkers {

  behavior of &amp;quot;Words counter&amp;quot;
  
  it should &amp;quot;have stable count, with generated RDDs&amp;quot; in {
     val stableProperty =
       forAll(RDDGenerator.genRDD[String](sc)(Arbitrary.arbitrary[String])) {
         rdd =&amp;gt; None === RDDComparisons.compare(WordCounter.count(rdd), WordCounter.count(rdd))
       }
     check(stableProperty)
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, &lt;code&gt;RddGenerator.genRDD[String]&lt;/code&gt; will generate RDDs on top of random Strings.&lt;/p&gt;

&lt;p&gt;We declare the property to have the same count result when executing twice the method.&lt;/p&gt;

&lt;p&gt;We then test the property with the ScalaCheck method.&lt;/p&gt;

&lt;p&gt;While not very relevant for the wordcount example, it allows to test your job logic against randomly generated data as input and therefore test the reliability of your code.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this article, we have seen how it is possible to refactor and test a Spark job. Testing your jobs will allow faster feedback when implementing them and you can even practice TDD.&lt;/p&gt;

&lt;p&gt;The next step would be to run the tests not only on a local cluster, but on a “production-like” cluster with more data on your continuous integration server. Simply override the &lt;code&gt;setMaster()&lt;/code&gt; value when configuring the &lt;code&gt;SparkContext&lt;/code&gt; to redirect to your test cluster.&lt;/p&gt;

&lt;p&gt;Finally, I definitely recommend you watch Holden Karau’s session on testing Spark recorded at the last Spark Summit (&lt;a href=&#34;https://www.youtube.com/watch?v=rOQEiTXNS0g&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;http://www.slideshare.net/SparkSummit/beyond-parallelize-and-collect-by-holden-karau&#34;&gt;slides&lt;/a&gt;).
You can find the code for these examples &lt;a href=&#34;https://github.com/raphaelbrugier/spark-testing-example&#34;&gt;on Github&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Retour sur QCon London 2012</title>
      <link>https://www.raphael-brugier.com/blog/retour-sur-qcon-london-2012/</link>
      <pubDate>Sun, 01 Apr 2012 18:58:03 +0000</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/retour-sur-qcon-london-2012/</guid>
      <description>&lt;p&gt;J&amp;rsquo;ai eu l&amp;rsquo;occasion cet année d&amp;rsquo;être envoyé par ma société, Objet Direct, assister aux 3 jours de la conférence QCon à Londres. Une excellente expérience, qui m&amp;rsquo;a permise de voir des présentations très variées aussi bien techniques que méthodologiques. Ça fait du bien de sortir de la mission pour aller voir ce qui se fait ailleurs et ça permet de revenir avec pleins d&amp;rsquo;idées de nouvelles et de motivation.&lt;/p&gt;

&lt;p&gt;J&amp;rsquo;espère pouvoir appliquer toutes les bonnes pratiques que j&amp;rsquo;ai appris là bas prochainement.&lt;/p&gt;

&lt;p&gt;J&amp;rsquo;ai écrit deux articles pour le blog d&amp;rsquo;Objet Direct :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Une synthèse succincte des 3 jours : &lt;a href=&#34;http://blog.objetdirect.com/divers/qcon-london-2012&#34;&gt;QCon London 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Un retour plus détaillé sur une présentation que j&amp;rsquo;ai particulièrement aimé sur GitHub : &amp;ldquo;&lt;a href=&#34;http://blog.objetdirect.com/divers/qcon-2012-how-github-works&#34;&gt;How GitHub Works&lt;/a&gt;&amp;ldquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bonne lecture !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wordpress login form with GWT and UiBinder</title>
      <link>https://www.raphael-brugier.com/blog/wordpress-login-form-with-gwt-and-uibinder/</link>
      <pubDate>Sun, 16 Jan 2011 16:40:14 +0000</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/wordpress-login-form-with-gwt-and-uibinder/</guid>
      <description>&lt;p&gt;Following my previous post, I have decided to rewrite the example of the Wordpress login form using only UiBinder and the ClientBundle/Css features that comes with GWT. This example demonstrate how you can use only htmlPanel and UiBinder to build a perfect pixel Ui just with hmtl/css and no gwt panel.&lt;/p&gt;

&lt;p&gt;You can &lt;a href=&#34;https://www.raphael-brugier.com/posts/wpLogin/WpLogin.html&#34;&gt;see the result there.&lt;/a&gt; Or you can &lt;a href=&#34;https://www.raphael-brugier.com/posts/wpLogin/wpLogin.zip&#34;&gt;download and run the eclipse project from there&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To start I&amp;rsquo;ve just copied the html from my wordpress login page into the htmlPanel. Then I&amp;rsquo;ve renamed all the &amp;ldquo;id&amp;rdquo; on the divs into class, because gwt will only compile the css declared in the UiBinder into class css. I&amp;rsquo;ve replaced the &amp;lt;form&amp;gt; by a &amp;lt;div&amp;gt;. I&amp;rsquo;ve added some more styles and the image directly in the UiBinder, so GWT can compress and obfuscate it. In the application.css I&amp;rsquo;ve just added two styles for the body.&lt;/p&gt;

&lt;p&gt;The main file that you want to see is WpForm.ui.xml :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;!DOCTYPE ui:UiBinder SYSTEM &amp;quot;http://dl.google.com/gwt/DTD/xhtml.ent&amp;quot;&amp;gt;
&amp;lt;ui:UiBinder xmlns:ui=&#39;urn:ui:com.google.gwt.uibinder&#39;
	xmlns:g=&#39;urn:import:com.google.gwt.user.client.ui&#39;&amp;gt;

	&amp;lt;ui:image field=&#39;logo&#39; src=&#39;logo-login.gif&#39; /&amp;gt;

	&amp;lt;ui:style&amp;gt;
		.form {
			margin-left: 8px;
			padding: 16px 16px 40px 16px;
			font-weight: normal;
			-moz-border-radius: 11px;
			-khtml-border-radius: 11px;
			-webkit-border-radius: 11px;
			border-radius: 5px;
			background: #fff;
			border: 1px solid #e5e5e5;
			-moz-box-shadow: rgba(200, 200, 200, 1) 0 4px 18px;
			-webkit-box-shadow: rgba(200, 200, 200, 1) 0 4px 18px;
			-khtml-box-shadow: rgba(200, 200, 200, 1) 0 4px 18px;
			box-shadow: rgba(200, 200, 200, 1) 0 4px 18px;
			font: 11px &amp;quot;Lucida Grande&amp;quot;, Verdana, Arial, &amp;quot;Bitstream Vera Sans&amp;quot;,
				sans-serif;
		}
		
		.form .forgetmenot {
			font-weight: normal;
			float: left;
			margin-bottom: 0;
		}
		
		.button-primary {
			background: #2379a1;
			color: #ffffff;
			font-family: &amp;quot;Lucida Grande&amp;quot;, Verdana, Arial, &amp;quot;Bitstream Vera Sans&amp;quot;,
				sans-serif;
			font-weight: bold;
			text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.3);
			padding: 3px 10px;
			border: none;
			font-size: 12px;
			border-width: 1px;
			border-style: solid;
			-moz-border-radius: 11px;
			-khtml-border-radius: 11px;
			-webkit-border-radius: 11px;
			border-radius: 11px;
			cursor: pointer;
			text-decoration: none;
			margin-top: -3px;
		}
		
		.login .form p {
			margin-bottom: 0px;
		}
		
		@external gwt-Label;
		.login .gwt-Label {
			color: #777;
			font-size: 13px;
		}
		
		.form .forgetmenot .gwt-Label {
			font-size: 11px;
			line-height: 19px;
			color: #777;
			float: right;
			margin-left: 5px;
		}
		
		.form .submit {
			float: right;
		}
		
		.form p {
			margin-bottom: 24px;
		}
		
		@sprite .login h1 a {
			gwt-image: &#39;logo&#39;;
			width: 326px;
			height: 67px;
			text-indent: -9999px;
			overflow: hidden;
			padding-bottom: 15px;
			display: block;
		}
		
		.nav {
			text-shadow: rgba(255, 255, 255, 1) 0 1px 0;
			margin: 0 0 0 8px;
			padding: 16px;
		}
		
		.nav a {
			color: #21759B;
		}
		
		.input {
			font-size: 24px;
			width: 97%;
			padding: 3px;
			margin-top: 2px;
			margin-right: 6px;
			margin-bottom: 16px;
			border: 1px solid #e5e5e5;
			background: #fbfbfb;
			color: #555;
		}
		
		.backtoblog {
			position: absolute;
			top: 0;
			left: 0;
			border-bottom: #c6c6c6 1px solid;
			background: #d9d9d9;
			background: -moz-linear-gradient(bottom, #d7d7d7, #e4e4e4);
			background: -webkit-gradient(linear, left bottom, left top, from(#d7d7d7),
				to(#e4e4e4) );
			height: 30px;
			width: 100%;
		}
		
		.backtoblog a {
			text-decoration: none;
			display: block;
			padding: 8px 0 0 15px;
			color: #464646;
		}
		
		.login {
			width: 320px;
			margin: 7em auto;
			padding-top: 30px;
		}
	&amp;lt;/ui:style&amp;gt;

	&amp;lt;g:HTMLPanel&amp;gt;
		&amp;lt;div class=&amp;quot;{style.login}&amp;quot;&amp;gt;
			&amp;lt;h1&amp;gt;
				&amp;lt;a title=&amp;quot;Propulsé par WordPress&amp;quot; href=&amp;quot;http://wordpress.org/&amp;quot;&amp;gt;Le blog de Raph&amp;lt;/a&amp;gt;
			&amp;lt;/h1&amp;gt;
			&amp;lt;div class=&amp;quot;{style.form}&amp;quot;&amp;gt;
				&amp;lt;p&amp;gt;
					&amp;lt;g:Label&amp;gt;Login&amp;lt;/g:Label&amp;gt;
					&amp;lt;br /&amp;gt;
					&amp;lt;g:TextBox styleName=&amp;quot;{style.input}&amp;quot; /&amp;gt;
				&amp;lt;/p&amp;gt;
				&amp;lt;p&amp;gt;
					&amp;lt;g:Label&amp;gt;Password&amp;lt;/g:Label&amp;gt;
					&amp;lt;br /&amp;gt;
					&amp;lt;g:TextBox styleName=&amp;quot;{style.input}&amp;quot; /&amp;gt;
				&amp;lt;/p&amp;gt;
				&amp;lt;p class=&amp;quot;{style.forgetmenot}&amp;quot;&amp;gt;
					&amp;lt;g:CheckBox&amp;gt;&amp;lt;/g:CheckBox&amp;gt;
					&amp;lt;g:Label&amp;gt; Se souvenir de moi&amp;lt;/g:Label&amp;gt;
				&amp;lt;/p&amp;gt;
				&amp;lt;p class=&amp;quot;{style.submit}&amp;quot;&amp;gt;
					&amp;lt;g:Button styleName=&amp;quot;{style.button-primary}&amp;quot;&amp;gt;Connect&amp;lt;/g:Button&amp;gt;

				&amp;lt;/p&amp;gt;
			&amp;lt;/div&amp;gt;

			&amp;lt;p class=&amp;quot;{style.nav}&amp;quot;&amp;gt;
				&amp;lt;a title=&amp;quot;Récupération de mot de passe&amp;quot;
					href=&amp;quot;http://{{ site.baseurl }}&amp;quot;&amp;gt;Mot de passe oublié ?&amp;lt;/a&amp;gt;
			&amp;lt;/p&amp;gt;
		&amp;lt;/div&amp;gt;
		&amp;lt;p class=&amp;quot;{style.backtoblog}&amp;quot;&amp;gt;
			&amp;lt;a title=&amp;quot;Êtes-vous perdu(e)?&amp;quot; href=&amp;quot;{{ site.baseurl }}&amp;quot;&amp;gt;← Retour sur Le blog de
				Raph&amp;lt;/a&amp;gt;
		&amp;lt;/p&amp;gt;
	&amp;lt;/g:HTMLPanel&amp;gt;
&amp;lt;/ui:UiBinder&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;WpLogin.css  :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;    * {
        margin: 0;
        padding: 0;
    }
    
    body {
        font: 11px &amp;quot;Lucida Grande&amp;quot;, Verdana, Arial, &amp;quot;Bitstream Vera Sans&amp;quot;,
            sans-serif !important;
            background-color: #f9f9f9 !important;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s all !  So now, please don&amp;rsquo;t follow the advices of writing all your html in a big static String and learn how to use the tool !&lt;/p&gt;

&lt;p&gt;Happy coding !&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.raphael-brugier.com/posts/wpLogin/WpLogin.html&#34;&gt;Result page&lt;/a&gt; Or you can &lt;a href=&#34;https://www.raphael-brugier.com/posts/wpLogin/wpLogin.zip&#34;&gt;Download the eclipse project&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GWT and HTML under control</title>
      <link>https://www.raphael-brugier.com/blog/gwt-and-html-under-control/</link>
      <pubDate>Sun, 16 Jan 2011 16:34:48 +0000</pubDate>
      
      <guid>https://www.raphael-brugier.com/blog/gwt-and-html-under-control/</guid>
      <description>&lt;p&gt;In the past weeks I&amp;rsquo;ve seen two articles about making clean design with GWT. That was really interesting, but something still stugling me a lot. Both authors agreed not to use  layout widgets provided by GWT to build UI neither UiBinder but use instead a big ugly static String to store an HTML code/&lt;/p&gt;

&lt;p&gt;Take a closer look at those two articles and come back later to find out a better way to improve your design based on GWT.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://i-proving.ca/space/Technologies/GWT/Lightweight+GWT+layouts&#34;&gt;Lightweight GWT layouts&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.zackgrossbart.com/hackito/tags-first-gwt/&#34;&gt;Tags first GWT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The purpose is to write a clean HTML with the least number of tags and have that HTML code under control with only CSS. I do agree that using the old GWT layout system is a really mess producing a lot of tables and ugly code.&lt;/p&gt;

&lt;p&gt;However, since GWT 2.0, Google brought us &lt;a href=&#34;http://www.google.com/events/io/2010/sessions/gwt-ui-overhaul.html&#34;&gt;a better layout system&lt;/a&gt; with the widgets positionned using only div and css. With this system you can normally reach a clean design without generating table tags and a lof HTML mess.&lt;/p&gt;

&lt;p&gt;But since the web is all about simple html and css, I agree with Zack Grossbart that you need to have a perfect control of your UI with the least html tags and just css.&lt;/p&gt;

&lt;p&gt;But with the version 2.0 GWT also introduces a wonderful feature called UiBinder. With UiBinder you can describe in a declarative way your design separated from your logic. You can especially use the HtmlPanel panel to mix up classic HTML/CSS and GWT widgets. This is exactly what they wanted to do ! Use classic HTML and CSS to describe the UI and GWT only for the control. By using UiBinder they can take advantage of a powerfull tool having a built-in feature to bind GWT widgets from the xml to the Java and many mores.&lt;/p&gt;

&lt;p&gt;Putting your html within a static String is not really that clean and maintainable. So please, for God sake, do not do it ! &lt;a href=&#34;http://code.google.com/intl/fr/webtoolkit/doc/latest/DevGuideUiBinder.html&#34;&gt;Learn the API&lt;/a&gt; and &lt;a href=&#34;http://www.google.com/events/io/2010/sessions.html#GWT&#34;&gt;follow the best practices&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[update] I&amp;rsquo;ve now coded a rewrite of the wordpress login page example just by using UiBinder and the HtmlPanel, see &lt;a href=&#34;https://www.raphael-brugier.com/blog/wordpress-login-form-with-gwt-and-uibinder/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>